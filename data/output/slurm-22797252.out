Requirement already satisfied: transformers in /home/s2976129/.local/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (4.15.0)
Requirement already satisfied: datasets in /home/s2976129/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.17.0)
Requirement already satisfied: sklearn in /home/s2976129/.local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.0)
Requirement already satisfied: filelock in /home/s2976129/.local/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (3.4.2)
Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/s2976129/.local/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (0.4.0)
Requirement already satisfied: numpy>=1.17 in /apps/skylake/software/SciPy-bundle/2019.10-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (1.17.3)
Requirement already satisfied: tqdm>=4.27 in /home/s2976129/.local/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (4.62.3)
Requirement already satisfied: sacremoses in /home/s2976129/.local/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (0.0.47)
Requirement already satisfied: pyyaml>=5.1 in /apps/skylake/software/PyYAML/5.1.2-GCCcore-8.3.0/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (5.1.2)
Requirement already satisfied: importlib-metadata; python_version < "3.8" in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (0.22)
Requirement already satisfied: requests in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (2.22.0)
Requirement already satisfied: packaging>=20.0 in /home/s2976129/.local/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (21.3)
Requirement already satisfied: regex!=2019.12.17 in /home/s2976129/.local/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (2021.11.10)
Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/s2976129/.local/lib/python3.7/site-packages (from transformers->-r requirements.txt (line 1)) (0.10.3)
Requirement already satisfied: multiprocess in /home/s2976129/.local/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 2)) (0.70.12.2)
Requirement already satisfied: aiohttp in /home/s2976129/.local/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 2)) (3.8.1)
Requirement already satisfied: dill in /home/s2976129/.local/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.4)
Requirement already satisfied: fsspec[http]>=2021.05.0 in /home/s2976129/.local/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 2)) (2022.1.0)
Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /home/s2976129/.local/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 2)) (6.0.1)
Requirement already satisfied: xxhash in /home/s2976129/.local/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 2)) (2.0.2)
Requirement already satisfied: pandas in /apps/skylake/software/SciPy-bundle/2019.10-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages (from datasets->-r requirements.txt (line 2)) (0.25.3)
Requirement already satisfied: scikit-learn in /home/s2976129/.local/lib/python3.7/site-packages (from sklearn->-r requirements.txt (line 3)) (1.0.2)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/s2976129/.local/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers->-r requirements.txt (line 1)) (4.0.1)
Requirement already satisfied: click in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (7.0)
Requirement already satisfied: joblib in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (0.13.2)
Requirement already satisfied: six in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from sacremoses->transformers->-r requirements.txt (line 1)) (1.12.0)
Requirement already satisfied: zipp>=0.5 in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from importlib-metadata; python_version < "3.8"->transformers->-r requirements.txt (line 1)) (0.6.0)
Requirement already satisfied: idna<2.9,>=2.5 in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2.8)
Requirement already satisfied: certifi>=2017.4.17 in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2019.9.11)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from requests->transformers->-r requirements.txt (line 1)) (1.25.3)
Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.0.4)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from packaging>=20.0->transformers->-r requirements.txt (line 1)) (2.4.2)
Requirement already satisfied: attrs>=17.3.0 in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (19.1.0)
Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/s2976129/.local/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (2.0.10)
Requirement already satisfied: multidict<7.0,>=4.5 in /home/s2976129/.local/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (5.2.0)
Requirement already satisfied: asynctest==0.13.0; python_version < "3.8" in /home/s2976129/.local/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (0.13.0)
Requirement already satisfied: frozenlist>=1.1.1 in /home/s2976129/.local/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.2.0)
Requirement already satisfied: aiosignal>=1.1.2 in /home/s2976129/.local/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.2.0)
Requirement already satisfied: yarl<2.0,>=1.0 in /home/s2976129/.local/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.7.2)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/s2976129/.local/lib/python3.7/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.2)
Requirement already satisfied: python-dateutil>=2.6.1 in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.0)
Requirement already satisfied: pytz>=2017.2 in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2019.2)
Requirement already satisfied: scipy>=1.1.0 in /apps/skylake/software/SciPy-bundle/2019.10-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 3)) (1.3.1)
Requirement already satisfied: threadpoolctl>=2.0.0 in /home/s2976129/.local/lib/python3.7/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 3)) (3.0.0)
Requirement already satisfied: more-itertools in /apps/skylake/software/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < "3.8"->transformers->-r requirements.txt (line 1)) (7.2.0)
Using custom data configuration default-cd3736568143f424
Working on file train_with_inverse (1 / 5)...
Downloading and preparing dataset csv/default to /home/s2976129/.cache/huggingface/datasets/csv/default-cd3736568143f424/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 3193.23it/s]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 87.10it/s]
Dataset csv downloaded and prepared to /home/s2976129/.cache/huggingface/datasets/csv/default-cd3736568143f424/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  7.53it/s]100%|██████████| 2/2 [00:00<00:00, 14.64it/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:02<00:05,  2.89s/ba] 67%|██████▋   | 2/3 [00:03<00:01,  1.26s/ba]100%|██████████| 3/3 [00:03<00:00,  1.33ba/s]100%|██████████| 3/3 [00:03<00:00,  1.05s/ba]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.24ba/s] 50%|█████     | 2/4 [00:00<00:00,  6.88ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  7.90ba/s]100%|██████████| 4/4 [00:00<00:00,  9.55ba/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running training *****
  Num examples = 2868
  Num Epochs = 4
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 1436
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8


Training completed. Do not forget to share your model on huggingface.co/models =)


Saving model checkpoint to /data/s2976129/shared-task-output/train_with_inverse/final-model
Configuration saved in /data/s2976129/shared-task-output/train_with_inverse/final-model/config.json
Model weights saved in /data/s2976129/shared-task-output/train_with_inverse/final-model/pytorch_model.bin
Using custom data configuration default-41c954d57801ff86
{'loss': 0.413, 'learning_rate': 3.259052924791087e-05, 'epoch': 1.39}
{'loss': 0.3402, 'learning_rate': 1.518105849582173e-05, 'epoch': 2.79}
{'eval_loss': 1.9699089527130127, 'eval_accuracy': 0.5543373105449855, 'eval_precision': 0.6120879120879121, 'eval_recall': 0.6223463687150838, 'eval_f1': 0.6171745152354571, 'eval_runtime': 37.0178, 'eval_samples_per_second': 83.77, 'eval_steps_per_second': 10.481, 'epoch': 2.79}
{'train_runtime': 463.6727, 'train_samples_per_second': 24.742, 'train_steps_per_second': 3.097, 'train_loss': 0.28724602935706006, 'epoch': 4.0}
Working on file train_with_new_hyponyms (2 / 5)...
Downloading and preparing dataset csv/default to /home/s2976129/.cache/huggingface/datasets/csv/default-41c954d57801ff86/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 6172.63it/s]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 84.73it/s]
Dataset csv downloaded and prepared to /home/s2976129/.cache/huggingface/datasets/csv/default-41c954d57801ff86/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 340.61it/s]
  0%|          | 0/5 [00:00<?, ?ba/s] 20%|██        | 1/5 [00:00<00:00,  6.42ba/s] 40%|████      | 2/5 [00:00<00:00,  7.84ba/s] 60%|██████    | 3/5 [00:00<00:00,  8.37ba/s] 80%|████████  | 4/5 [00:00<00:00,  8.42ba/s]100%|██████████| 5/5 [00:00<00:00,  8.55ba/s]100%|██████████| 5/5 [00:00<00:00,  8.26ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  7.19ba/s] 50%|█████     | 2/4 [00:00<00:00,  8.16ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.49ba/s]100%|██████████| 4/4 [00:00<00:00, 10.50ba/s]
loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/s2976129/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.15.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/s2976129/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running training *****
  Num examples = 4956
  Num Epochs = 4
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 2480
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8


Training completed. Do not forget to share your model on huggingface.co/models =)


Saving model checkpoint to /data/s2976129/shared-task-output/train_with_new_hyponyms/final-model
Configuration saved in /data/s2976129/shared-task-output/train_with_new_hyponyms/final-model/config.json
Model weights saved in /data/s2976129/shared-task-output/train_with_new_hyponyms/final-model/pytorch_model.bin
Using custom data configuration default-132fe94d897f120a
{'loss': 0.5431, 'learning_rate': 3.991935483870968e-05, 'epoch': 0.81}
{'loss': 0.2862, 'learning_rate': 2.9838709677419357e-05, 'epoch': 1.61}
{'eval_loss': 1.9921311140060425, 'eval_accuracy': 0.4743631086746211, 'eval_precision': 0.5426439232409381, 'eval_recall': 0.5687150837988827, 'eval_f1': 0.5553737043098744, 'eval_runtime': 37.0076, 'eval_samples_per_second': 83.794, 'eval_steps_per_second': 10.484, 'epoch': 1.61}
{'loss': 0.1495, 'learning_rate': 1.975806451612903e-05, 'epoch': 2.42}
{'loss': 0.0843, 'learning_rate': 9.67741935483871e-06, 'epoch': 3.23}
{'eval_loss': 3.738069534301758, 'eval_accuracy': 0.38439213157046115, 'eval_precision': 0.45824561403508773, 'eval_recall': 0.364804469273743, 'eval_f1': 0.4062208398133748, 'eval_runtime': 36.0756, 'eval_samples_per_second': 85.958, 'eval_steps_per_second': 10.755, 'epoch': 3.23}
{'train_runtime': 799.8784, 'train_samples_per_second': 24.784, 'train_steps_per_second': 3.1, 'train_loss': 0.21953265436234012, 'epoch': 4.0}
Working on file train_base (3 / 5)...
Downloading and preparing dataset csv/default to /home/s2976129/.cache/huggingface/datasets/csv/default-132fe94d897f120a/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 7339.11it/s]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 95.80it/s]
Dataset csv downloaded and prepared to /home/s2976129/.cache/huggingface/datasets/csv/default-132fe94d897f120a/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 300.46it/s]
  0%|          | 0/3 [00:00<?, ?ba/s] 33%|███▎      | 1/3 [00:00<00:00,  6.73ba/s]100%|██████████| 3/3 [00:00<00:00,  9.78ba/s]100%|██████████| 3/3 [00:00<00:00,  9.35ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  5.04ba/s] 50%|█████     | 2/4 [00:00<00:00,  7.08ba/s]100%|██████████| 4/4 [00:00<00:00, 11.07ba/s]100%|██████████| 4/4 [00:00<00:00,  9.52ba/s]
loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/s2976129/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.15.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/s2976129/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running training *****
  Num examples = 2736
  Num Epochs = 4
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 1368
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8


Training completed. Do not forget to share your model on huggingface.co/models =)


Saving model checkpoint to /data/s2976129/shared-task-output/train_base/final-model
Configuration saved in /data/s2976129/shared-task-output/train_base/final-model/config.json
Model weights saved in /data/s2976129/shared-task-output/train_base/final-model/pytorch_model.bin
Using custom data configuration default-d76c5ca67a998559
{'loss': 0.3842, 'learning_rate': 3.172514619883041e-05, 'epoch': 1.46}
{'loss': 0.1473, 'learning_rate': 1.3450292397660819e-05, 'epoch': 2.92}
{'eval_loss': 2.5075109004974365, 'eval_accuracy': 0.5124153498871332, 'eval_precision': 0.5966620305980529, 'eval_recall': 0.4793296089385475, 'eval_f1': 0.5315985130111526, 'eval_runtime': 36.6511, 'eval_samples_per_second': 84.609, 'eval_steps_per_second': 10.586, 'epoch': 2.92}
{'train_runtime': 441.9978, 'train_samples_per_second': 24.76, 'train_steps_per_second': 3.095, 'train_loss': 0.2013314599879304, 'epoch': 4.0}
Working on file train_with_new_hyponyms_with_new_templates (4 / 5)...
Downloading and preparing dataset csv/default to /home/s2976129/.cache/huggingface/datasets/csv/default-d76c5ca67a998559/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 6359.82it/s]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 65.26it/s]
Dataset csv downloaded and prepared to /home/s2976129/.cache/huggingface/datasets/csv/default-d76c5ca67a998559/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 341.56it/s]
  0%|          | 0/19 [00:00<?, ?ba/s]  5%|▌         | 1/19 [00:00<00:02,  7.07ba/s] 11%|█         | 2/19 [00:00<00:02,  7.96ba/s] 16%|█▌        | 3/19 [00:00<00:02,  7.77ba/s] 21%|██        | 4/19 [00:00<00:01,  8.30ba/s] 26%|██▋       | 5/19 [00:00<00:01,  8.58ba/s] 32%|███▏      | 6/19 [00:00<00:01,  8.65ba/s] 37%|███▋      | 7/19 [00:00<00:01,  8.77ba/s] 42%|████▏     | 8/19 [00:00<00:01,  8.15ba/s] 47%|████▋     | 9/19 [00:01<00:01,  8.15ba/s] 53%|█████▎    | 10/19 [00:01<00:01,  8.08ba/s] 58%|█████▊    | 11/19 [00:01<00:01,  6.58ba/s] 63%|██████▎   | 12/19 [00:01<00:00,  7.19ba/s] 68%|██████▊   | 13/19 [00:01<00:00,  7.73ba/s] 74%|███████▎  | 14/19 [00:01<00:00,  8.02ba/s] 79%|███████▉  | 15/19 [00:01<00:00,  8.26ba/s] 84%|████████▍ | 16/19 [00:01<00:00,  8.42ba/s] 89%|████████▉ | 17/19 [00:02<00:00,  8.44ba/s] 95%|█████████▍| 18/19 [00:02<00:00,  8.50ba/s]100%|██████████| 19/19 [00:02<00:00,  8.60ba/s]100%|██████████| 19/19 [00:02<00:00,  8.11ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.17ba/s] 50%|█████     | 2/4 [00:00<00:00,  7.68ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  8.31ba/s]100%|██████████| 4/4 [00:00<00:00, 10.07ba/s]
loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/s2976129/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.15.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/s2976129/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running training *****
  Num examples = 18831
  Num Epochs = 4
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 9416
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
Saving model checkpoint to /data/s2976129/shared-task-output/train_with_new_hyponyms_with_new_templates/checkpoint-5000
Configuration saved in /data/s2976129/shared-task-output/train_with_new_hyponyms_with_new_templates/checkpoint-5000/config.json
Model weights saved in /data/s2976129/shared-task-output/train_with_new_hyponyms_with_new_templates/checkpoint-5000/pytorch_model.bin
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from /data/s2976129/shared-task-output/train_with_new_hyponyms_with_new_templates/checkpoint-5000 (score: 2.463951826095581).
Saving model checkpoint to /data/s2976129/shared-task-output/train_with_new_hyponyms_with_new_templates/final-model
Configuration saved in /data/s2976129/shared-task-output/train_with_new_hyponyms_with_new_templates/final-model/config.json
Model weights saved in /data/s2976129/shared-task-output/train_with_new_hyponyms_with_new_templates/final-model/pytorch_model.bin
Using custom data configuration default-041a870b7e843510
{'loss': 0.573, 'learning_rate': 4.734494477485132e-05, 'epoch': 0.21}
{'loss': 0.4061, 'learning_rate': 4.4689889549702635e-05, 'epoch': 0.42}
{'eval_loss': 1.801079511642456, 'eval_accuracy': 0.5172524991938084, 'eval_precision': 0.8262806236080178, 'eval_recall': 0.20726256983240224, 'eval_f1': 0.331397945511389, 'eval_runtime': 36.6892, 'eval_samples_per_second': 84.521, 'eval_steps_per_second': 10.575, 'epoch': 0.42}
{'loss': 0.3639, 'learning_rate': 4.2034834324553954e-05, 'epoch': 0.64}
{'loss': 0.3258, 'learning_rate': 3.937977909940527e-05, 'epoch': 0.85}
{'eval_loss': 2.22963547706604, 'eval_accuracy': 0.5804579168010319, 'eval_precision': 0.8419580419580419, 'eval_recall': 0.33631284916201115, 'eval_f1': 0.4806387225548902, 'eval_runtime': 36.7549, 'eval_samples_per_second': 84.37, 'eval_steps_per_second': 10.556, 'epoch': 0.85}
{'loss': 0.3069, 'learning_rate': 3.672472387425659e-05, 'epoch': 1.06}
{'loss': 0.2605, 'learning_rate': 3.4069668649107906e-05, 'epoch': 1.27}
{'eval_loss': 2.4121739864349365, 'eval_accuracy': 0.5033860045146726, 'eval_precision': 0.7306273062730627, 'eval_recall': 0.2212290502793296, 'eval_f1': 0.33962264150943394, 'eval_runtime': 36.6875, 'eval_samples_per_second': 84.525, 'eval_steps_per_second': 10.576, 'epoch': 1.27}
{'loss': 0.2066, 'learning_rate': 3.141461342395922e-05, 'epoch': 1.49}
{'loss': 0.1155, 'learning_rate': 2.875955819881054e-05, 'epoch': 1.7}
{'eval_loss': 2.288313865661621, 'eval_accuracy': 0.6343115124153499, 'eval_precision': 0.8430962343096234, 'eval_recall': 0.45027932960893857, 'eval_f1': 0.5870356882738529, 'eval_runtime': 36.0945, 'eval_samples_per_second': 85.913, 'eval_steps_per_second': 10.75, 'epoch': 1.7}
{'loss': 0.0947, 'learning_rate': 2.6104502973661855e-05, 'epoch': 1.91}
{'loss': 0.0505, 'learning_rate': 2.344944774851317e-05, 'epoch': 2.12}
{'eval_loss': 2.463951826095581, 'eval_accuracy': 0.6668816510802967, 'eval_precision': 0.7900383141762453, 'eval_recall': 0.575977653631285, 'eval_f1': 0.6662358642972537, 'eval_runtime': 36.021, 'eval_samples_per_second': 86.089, 'eval_steps_per_second': 10.771, 'epoch': 2.12}
{'loss': 0.0403, 'learning_rate': 2.0794392523364487e-05, 'epoch': 2.34}
{'loss': 0.0295, 'learning_rate': 1.8139337298215803e-05, 'epoch': 2.55}
{'eval_loss': 3.48382306098938, 'eval_accuracy': 0.6043211867139633, 'eval_precision': 0.8670143415906127, 'eval_recall': 0.3715083798882682, 'eval_f1': 0.5201407899882674, 'eval_runtime': 36.0761, 'eval_samples_per_second': 85.957, 'eval_steps_per_second': 10.755, 'epoch': 2.55}
{'loss': 0.0393, 'learning_rate': 1.5484282073067123e-05, 'epoch': 2.76}
{'loss': 0.0263, 'learning_rate': 1.2829226847918437e-05, 'epoch': 2.97}
{'eval_loss': 2.586366653442383, 'eval_accuracy': 0.7126733311834892, 'eval_precision': 0.8799661876584953, 'eval_recall': 0.5815642458100558, 'eval_f1': 0.7003027245206862, 'eval_runtime': 36.0115, 'eval_samples_per_second': 86.111, 'eval_steps_per_second': 10.774, 'epoch': 2.97}
{'train_runtime': 2305.749, 'train_samples_per_second': 32.668, 'train_steps_per_second': 4.084, 'train_loss': 0.2027850971221924, 'epoch': 2.97}
Working on file train_with_pegasus (5 / 5)...
Downloading and preparing dataset csv/default to /home/s2976129/.cache/huggingface/datasets/csv/default-041a870b7e843510/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 7364.89it/s]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 71.94it/s]
Dataset csv downloaded and prepared to /home/s2976129/.cache/huggingface/datasets/csv/default-041a870b7e843510/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  2.18it/s]100%|██████████| 2/2 [00:00<00:00,  4.33it/s]
  0%|          | 0/20 [00:00<?, ?ba/s]  5%|▌         | 1/20 [00:01<00:20,  1.10s/ba] 10%|█         | 2/20 [00:01<00:10,  1.76ba/s] 20%|██        | 4/20 [00:01<00:04,  3.66ba/s] 30%|███       | 6/20 [00:01<00:02,  5.25ba/s] 35%|███▌      | 7/20 [00:02<00:03,  3.80ba/s] 40%|████      | 8/20 [00:02<00:02,  4.53ba/s] 50%|█████     | 10/20 [00:02<00:01,  6.04ba/s] 60%|██████    | 12/20 [00:02<00:01,  7.24ba/s] 65%|██████▌   | 13/20 [00:02<00:00,  7.31ba/s] 70%|███████   | 14/20 [00:02<00:00,  7.76ba/s] 80%|████████  | 16/20 [00:03<00:00,  8.53ba/s] 85%|████████▌ | 17/20 [00:03<00:00,  8.82ba/s] 95%|█████████▌| 19/20 [00:03<00:00,  8.32ba/s]100%|██████████| 20/20 [00:03<00:00,  5.72ba/s]
  0%|          | 0/4 [00:00<?, ?ba/s] 25%|██▌       | 1/4 [00:00<00:00,  6.84ba/s] 75%|███████▌  | 3/4 [00:00<00:00,  9.02ba/s]100%|██████████| 4/4 [00:00<00:00, 11.08ba/s]
loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /home/s2976129/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.15.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /home/s2976129/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running training *****
  Num examples = 19484
  Num Epochs = 4
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 9744
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
Saving model checkpoint to /data/s2976129/shared-task-output/train_with_pegasus/checkpoint-5000
Configuration saved in /data/s2976129/shared-task-output/train_with_pegasus/checkpoint-5000/config.json
Model weights saved in /data/s2976129/shared-task-output/train_with_pegasus/checkpoint-5000/pytorch_model.bin
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8
The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text, id.
***** Running Evaluation *****
  Num examples = 3101
  Batch size = 8


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from /data/s2976129/shared-task-output/train_with_pegasus/checkpoint-5000 (score: 3.9052610397338867).
Saving model checkpoint to /data/s2976129/shared-task-output/train_with_pegasus/final-model
Configuration saved in /data/s2976129/shared-task-output/train_with_pegasus/final-model/config.json
Model weights saved in /data/s2976129/shared-task-output/train_with_pegasus/final-model/pytorch_model.bin
{'loss': 0.4095, 'learning_rate': 4.743431855500821e-05, 'epoch': 0.21}
{'loss': 0.2137, 'learning_rate': 4.486863711001642e-05, 'epoch': 0.41}
{'eval_loss': 1.5274150371551514, 'eval_accuracy': 0.6830054821025475, 'eval_precision': 0.654302103250478, 'eval_recall': 0.9558659217877095, 'eval_f1': 0.776844494892168, 'eval_runtime': 35.8662, 'eval_samples_per_second': 86.46, 'eval_steps_per_second': 10.818, 'epoch': 0.41}
{'loss': 0.1689, 'learning_rate': 4.230295566502464e-05, 'epoch': 0.62}
{'loss': 0.117, 'learning_rate': 3.9737274220032846e-05, 'epoch': 0.82}
{'eval_loss': 3.3940320014953613, 'eval_accuracy': 0.5304740406320542, 'eval_precision': 0.5902702702702702, 'eval_recall': 0.6100558659217877, 'eval_f1': 0.5999999999999999, 'eval_runtime': 36.0431, 'eval_samples_per_second': 86.036, 'eval_steps_per_second': 10.765, 'epoch': 0.82}
{'loss': 0.1055, 'learning_rate': 3.7171592775041055e-05, 'epoch': 1.03}
{'loss': 0.0965, 'learning_rate': 3.4605911330049265e-05, 'epoch': 1.23}
{'eval_loss': 2.207704544067383, 'eval_accuracy': 0.689455014511448, 'eval_precision': 0.6738125262715426, 'eval_recall': 0.8955307262569833, 'eval_f1': 0.7690093547613336, 'eval_runtime': 35.8585, 'eval_samples_per_second': 86.479, 'eval_steps_per_second': 10.82, 'epoch': 1.23}
{'loss': 0.0886, 'learning_rate': 3.2040229885057474e-05, 'epoch': 1.44}
{'loss': 0.0747, 'learning_rate': 2.947454844006568e-05, 'epoch': 1.64}
{'eval_loss': 2.2081410884857178, 'eval_accuracy': 0.6807481457594324, 'eval_precision': 0.6705029838022165, 'eval_recall': 0.8787709497206704, 'eval_f1': 0.7606382978723404, 'eval_runtime': 36.0415, 'eval_samples_per_second': 86.04, 'eval_steps_per_second': 10.765, 'epoch': 1.64}
{'loss': 0.0827, 'learning_rate': 2.6908866995073896e-05, 'epoch': 1.85}
{'loss': 0.0803, 'learning_rate': 2.4343185550082105e-05, 'epoch': 2.05}
{'eval_loss': 3.9052610397338867, 'eval_accuracy': 0.4601741373750403, 'eval_precision': 0.5320088300220751, 'eval_recall': 0.5385474860335195, 'eval_f1': 0.535258189894503, 'eval_runtime': 35.9675, 'eval_samples_per_second': 86.217, 'eval_steps_per_second': 10.788, 'epoch': 2.05}
{'loss': 0.0585, 'learning_rate': 2.1777504105090314e-05, 'epoch': 2.26}
{'loss': 0.0568, 'learning_rate': 1.921182266009852e-05, 'epoch': 2.46}
{'eval_loss': 4.168469429016113, 'eval_accuracy': 0.27249274427604, 'eval_precision': 0.26272912423625255, 'eval_recall': 0.1441340782122905, 'eval_f1': 0.18614718614718617, 'eval_runtime': 35.9488, 'eval_samples_per_second': 86.262, 'eval_steps_per_second': 10.793, 'epoch': 2.46}
{'loss': 0.0558, 'learning_rate': 1.6646141215106733e-05, 'epoch': 2.67}
{'loss': 0.0471, 'learning_rate': 1.4080459770114942e-05, 'epoch': 2.87}
{'eval_loss': 3.473564386367798, 'eval_accuracy': 0.5769106739761367, 'eval_precision': 0.6192614770459082, 'eval_recall': 0.6932960893854748, 'eval_f1': 0.6541908276225619, 'eval_runtime': 35.8769, 'eval_samples_per_second': 86.434, 'eval_steps_per_second': 10.815, 'epoch': 2.87}
{'loss': 0.043, 'learning_rate': 1.1514778325123153e-05, 'epoch': 3.08}
{'loss': 0.0453, 'learning_rate': 8.949096880131364e-06, 'epoch': 3.28}
{'eval_loss': 2.7766735553741455, 'eval_accuracy': 0.6559174459851661, 'eval_precision': 0.6616003576218149, 'eval_recall': 0.8268156424581006, 'eval_f1': 0.7350384901912094, 'eval_runtime': 35.9838, 'eval_samples_per_second': 86.178, 'eval_steps_per_second': 10.783, 'epoch': 3.28}
{'loss': 0.0345, 'learning_rate': 6.383415435139574e-06, 'epoch': 3.49}
{'loss': 0.0447, 'learning_rate': 3.817733990147783e-06, 'epoch': 3.69}
{'eval_loss': 4.0929975509643555, 'eval_accuracy': 0.5185424056755885, 'eval_precision': 0.5817281232801321, 'eval_recall': 0.5905027932960893, 'eval_f1': 0.5860826171333517, 'eval_runtime': 35.9468, 'eval_samples_per_second': 86.266, 'eval_steps_per_second': 10.794, 'epoch': 3.69}
{'loss': 0.0304, 'learning_rate': 1.2520525451559936e-06, 'epoch': 3.9}
{'train_runtime': 3115.1686, 'train_samples_per_second': 25.018, 'train_steps_per_second': 3.128, 'train_loss': 0.09595498437756192, 'epoch': 4.0}


###############################################################################
Peregrine Cluster
Job 22797252 for user 's2976129'
Finished at: Sat Jan 15 17:46:23 CET 2022

Job details:
============

Job ID              : 22797252
Name                : data_experiment_job
User                : s2976129
Partition           : gpu
Nodes               : pg-gpu42
Number of Nodes     : 1
Cores               : 12
Number of Tasks     : 1
State               : COMPLETED
Submit              : 2022-01-15T15:45:24
Start               : 2022-01-15T15:45:24
End                 : 2022-01-15T17:46:22
Reserved walltime   : 1-00:00:00
Used walltime       :   02:00:58
Used CPU time       :   01:59:23 (efficiency:  8.22%)
% User (Computation): 65.29%
% System (I/O)      : 34.71%
Mem reserved        : 8000M/node
Max Mem (Node/step) : 2.19G (pg-gpu42, per node)
Full Max Mem usage  : 2.19G
Total Disk Read     : 8.09M
Total Disk Write    : 34.50K
Average GPU usage   : 88.5% (pg-gpu42)


Acknowledgements:
=================

Please see this page for information about acknowledging Peregrine in your publications:

https://wiki.hpc.rug.nl/peregrine/introduction/scientific_output

################################################################################
